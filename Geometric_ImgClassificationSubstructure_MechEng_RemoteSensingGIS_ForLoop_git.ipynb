{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "661bf160-9c37-4dac-beb5-e75cb109348b",
   "metadata": {},
   "source": [
    "# <u>Geometric image classification substructure of the Hornsund Fjord: Mechanical Engineering applications in remote sensing and Geography Information Systems (GIS) (For Loop Version) </u>  <br /> <sub> <span style=\"font-size:smaller;\"> Jullian C.B. Williams </span> </sub> <br /> <sub> <span style=\"font-size:smaller;\"> *Polish Academy of Sciences, Institute of Geophysics, Warsaw, Poland* </span> </sub>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d605af-6e21-4476-85e8-52484840a944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install shapely\n",
    "# !pip install geopandas\n",
    "# !pip install fiona\n",
    "# !pip install rasterstats\n",
    "# !pip install spectral\n",
    "# !pip install pyarrow\n",
    "# !pip install chardet\n",
    "# !pip install fsspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e3beb26-c179-4529-b235-024472ded431",
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji #<---- Death and taxes are for certain. Try to have fun before you're 6 feet too far under to.\n",
    "from rich.console import Console #<---- Ditto.\n",
    "\n",
    "import fiona\n",
    "import random\n",
    "import warnings\n",
    "import argparse, os\n",
    "import pyarrow as pa\n",
    "from shapely.geometry import box\n",
    "from shapely.geometry import shape\n",
    "from shapely.geometry import Polygon\n",
    "from shapely.affinity import translate\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from osgeo import ogr\n",
    "from osgeo import gdal #<---- to geospatial env.\n",
    "import geopandas as gpd\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import rasterio.mask\n",
    "import rasterio as rio\n",
    "from rasterio.plot import show\n",
    "from rasterio.plot import show_hist \n",
    "from rasterstats import zonal_stats\n",
    "from rasterio.features import shapes\n",
    "from rasterio.plot import adjust_band\n",
    "from rasterio.features import rasterize\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.image\n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "from matplotlib import patches as mpatches\n",
    "from matplotlib.colors import ListedColormap\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "from spectral import *\n",
    "from sklearn import svm\n",
    "from scipy import stats\n",
    "from scipy import ndimage\n",
    "from sklearn.svm import SVC\n",
    "from scipy.ndimage import zoom\n",
    "from scipy.ndimage import sobel\n",
    "from scipy.stats import entropy\n",
    "import spectral.io.envi as envi #run PCA\n",
    "from sklearn.svm import LinearSVC\n",
    "from skimage import io, transform # from scipy.misc import imresize  <---- here...we...go..\n",
    "from skimage.morphology import disk #manage disk while running.\n",
    "from skimage.segmentation import slic\n",
    "from skimage.util import img_as_ubyte #Avoid precision loss converting image of type float32 to uint8 as required by rank filters.\n",
    "from skimage.filters.rank import entropy #for GLCM entropy.\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from skimage.segmentation import chan_vese\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage.feature import graycomatrix, graycoprops # from skimage.feature import greycomatrix, greycoprops <--- depreciated or..? lang.\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5e607a-a5b2-4e94-bed4-9b7a5518c23f",
   "metadata": {},
   "source": [
    "## Loop SVM raster to vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114f84e9-8e4f-4163-82ee-8f44f317174b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"C:/Users/jullian.williams/Desktop/HIRLOMAP/frnd/ALL_SAT/composite/RS2/GLCM_stack_test/\"\n",
    "output_dir = \"C:/Users/jullian.williams/Desktop/HIRLOMAP/frnd/ALL_SAT/composite/RS2/Winter_reclass_test/\"\n",
    "\n",
    "raster_path = \"C:/Users/jullian.williams/Desktop/HIRLOMAP/output_bandsSVMtrain_Pair.tif\"\n",
    "with rio.open(raster_path) as src:\n",
    "    image = src.read()  # Read all bands\n",
    "    profile = src.profile  # Save metadata\n",
    "    b1 = src.read(1)\n",
    "    b2 = src.read(2)\n",
    "    b3 = src.read(3)\n",
    "    b4 = src.read(4)\n",
    "    b5 = src.read(5)\n",
    "    b6 = src.read(6)\n",
    "    b7 = src.read(7)\n",
    "    b8 = src.read(8)\n",
    "    b9 = src.read(9)\n",
    "    b10 = src.read(10)\n",
    "    b11 = src.read(11)\n",
    "    b12 = src.read(12)\n",
    "    b13 = src.read(13)\n",
    "    b14 = src.read(14)\n",
    "\n",
    "    image = np.moveaxis(image, 0, -1)  # Rearrange to (rows, cols, bands)\n",
    "\n",
    "    bands = np.dstack((b1,b2,b3,b4,b5,b6,b7,b8,b9,b10,b11,b12,b13,b14))  # <-- Why must b1 be alpha? Odd.\n",
    "    bands = bands.reshape(int(np.prod(bands.shape)/14),14)\n",
    "\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Loop through all .tif files in the input directory\n",
    "    for subdir, _, files in os.walk(input_dir):\n",
    "        for filename in files: \n",
    "            # Check if the file is an image (e.g., .jpg, .png)\n",
    "            if filename.endswith(('.tif')):\n",
    "                input_path = os.path.join(subdir, filename)\n",
    "                output_path = os.path.join(output_dir, filename.replace(\".tif\", \".shp\"))\n",
    "                 \n",
    "            with rio.open(input_path) as src_:\n",
    "                profile_ = src_.profile  # Save metadata\n",
    "                transform_ = src_.transform\n",
    "                crs_=src_.crs\n",
    "                b1_ = src_.read(1)\n",
    "                b2_ = src_.read(2)\n",
    "                b3_ = src_.read(3)\n",
    "                b4_ = src_.read(4)\n",
    "                b5_ = src_.read(5)\n",
    "                b6_ = src_.read(6)\n",
    "                b7_ = src_.read(7)\n",
    "                b8_ = src_.read(8)\n",
    "                b9_ = src_.read(9)\n",
    "                b10_ = src_.read(10)\n",
    "                b11_ = src_.read(11)\n",
    "                b12_ = src_.read(12)\n",
    "                b13_ = src_.read(13)\n",
    "                b14_ = src_.read(14)\n",
    "            \n",
    "                b1_ = np.where(np.isinf(b1_), np.nan, b1_)\n",
    "                b2_ = np.where(np.isinf(b2_), np.nan, b2_)          \n",
    "                b3_ = np.where(np.isinf(b3_), np.nan, b3_)\n",
    "                b4_ = np.where(np.isinf(b4_), np.nan, b4_)\n",
    "                b5_ = np.where(np.isinf(b5_), np.nan, b5_)\n",
    "                b6_ = np.where(np.isinf(b6_), np.nan, b6_)\n",
    "                b7_ = np.where(np.isinf(b7_), np.nan, b7_)\n",
    "                b8_ = np.where(np.isinf(b8_), np.nan, b8_)\n",
    "                b9_ = np.where(np.isinf(b9_), np.nan, b9_)\n",
    "                b10_ = np.where(np.isinf(b10_), np.nan, b10_)\n",
    "                b11_ = np.where(np.isinf(b11_), np.nan, b11_)\n",
    "                b12_ = np.where(np.isinf(b12_), np.nan, b12_)\n",
    "                b13_ = np.where(np.isinf(b13_), np.nan, b13_)\n",
    "                b14_ = np.where(np.isinf(b14_), np.nan, b14_)\n",
    "                            \n",
    "                nan1 = 9\n",
    "                nan2 = 81\n",
    "                \n",
    "                b1_ = np.nan_to_num(b1_, nan=nan1)\n",
    "                b2_ = np.nan_to_num(b2_, nan=nan1)\n",
    "                b3_ = np.nan_to_num(b3_, nan=nan1)\n",
    "                b4_ = np.nan_to_num(b4_, nan=nan1)\n",
    "                b5_ = np.nan_to_num(b5_, nan=nan2)\n",
    "                b6_ = np.nan_to_num(b6_, nan=nan2)\n",
    "                b7_ = np.nan_to_num(b7_, nan=nan1)\n",
    "                b8_ = np.nan_to_num(b8_, nan=nan1)\n",
    "                b9_ = np.nan_to_num(b9_, nan=nan1)\n",
    "                b10_ = np.nan_to_num(b10_, nan=nan1)\n",
    "                b11_ = np.nan_to_num(b11_, nan=nan1)\n",
    "                b12_ = np.nan_to_num(b12_, nan=nan1)\n",
    "                b13_ = np.nan_to_num(b13_, nan=nan2)\n",
    "                b14_ = np.nan_to_num(b14_, nan=nan2)\n",
    "            \n",
    "            \n",
    "                mask1 = (b1_ ==9) & (b1_== 81)\n",
    "                mask2 = (b2_ ==9) & (b2_== 81)\n",
    "                mask3 = (b3_ ==9) & (b3_== 81)\n",
    "                mask4 = (b4_ ==9) & (b4_== 81)\n",
    "                mask5 = (b5_ ==9) & (b5_== 81)\n",
    "                mask6 = (b6_ ==9) & (b6_== 81)\n",
    "                mask7 = (b7_ ==9) & (b7_== 81)\n",
    "                mask8 = (b8_ ==9) & (b8_== 81)\n",
    "                mask9 = (b9_ ==9) & (b9_== 81)\n",
    "                mask10 = (b10_ ==9) & (b10_== 81)\n",
    "                mask11 = (b11_ ==9) & (b11_== 81)\n",
    "                mask12 = (b12_ ==9) & (b12_== 81)\n",
    "                mask13 = (b13_ ==9) & (b13_== 81)\n",
    "                mask14 = (b14_ ==9) & (b14_== 81)\n",
    "                \n",
    "                b1_ = np.ma.masked_array(b1_, mask1)\n",
    "                b2_ = np.ma.masked_array(b2_, mask2)\n",
    "                b3_ = np.ma.masked_array(b3_, mask3)\n",
    "                b4_ = np.ma.masked_array(b4_, mask4)\n",
    "                b5_ = np.ma.masked_array(b5_, mask5)\n",
    "                b6_ = np.ma.masked_array(b6_, mask6)\n",
    "                b7_ = np.ma.masked_array(b7_, mask7)\n",
    "                b8_ = np.ma.masked_array(b8_, mask8)\n",
    "                b9_ = np.ma.masked_array(b9_, mask9)\n",
    "                b10_ = np.ma.masked_array(b10_, mask10)\n",
    "                b11_ = np.ma.masked_array(b11_, mask11)\n",
    "                b12_ = np.ma.masked_array(b12_, mask12)\n",
    "                b13_ = np.ma.masked_array(b13_, mask13)\n",
    "                b14_ = np.ma.masked_array(b14_, mask14)\n",
    "            \n",
    "                # image_ = np.moveaxis(image_, 0, -1)  # Rearrange to (rows, cols, bands)\n",
    "            \n",
    "                bands_ = np.dstack((b1_,b2_,b3_,b4_,b5_,b6_,b7_,b8_,b9_,b10_,b11_,b12_,b13_,b14_))  # <-- Why must b1 be alpha? Odd.\n",
    "                bands_ = bands_.reshape(int(np.prod(bands_.shape)/14),14)\n",
    "            \n",
    "            \n",
    "            # Load the shapefile\n",
    "            shapefile_path = \"C:/Users/jullian.williams/Desktop/HIRLOMAP/ROI/PTS_MERGED/HS_ice_water_MASTER_.shp\"\n",
    "            shapefile = gpd.read_file(shapefile_path)\n",
    "            \n",
    "            # Extract training data\n",
    "            # Assuming the shapefile has a 'class' column for labels\n",
    "            training_data = []\n",
    "            labels = []\n",
    "            \n",
    "            for _, row in shapefile.iterrows():\n",
    "                geom = row.geometry\n",
    "                label = row['Class_']\n",
    "                mask = rio.features.geometry_mask([geom], transform=profile['transform'], invert=True, out_shape=image.shape[:2])\n",
    "                pixels = image[mask]\n",
    "                training_data.append(pixels)\n",
    "                labels.extend([label] * len(pixels))\n",
    "            \n",
    "            training_data = np.vstack(training_data)\n",
    "            labels = np.array(labels)\n",
    "            \n",
    "            #Train the SVM model\n",
    "            X_train, X_test, y_train, y_test = train_test_split(training_data, labels, test_size=0.3, random_state=42)\n",
    "            clf = make_pipeline(StandardScaler(),LinearSVC(random_state=0, tol=1e-3))\n",
    "            clf.fit(X_train, y_train)\n",
    "            \n",
    "            #Classify the image\n",
    "            predicted = clf.predict(bands_) #Predict using the model\n",
    "            class_image = predicted.reshape(b1_.shape)\n",
    "            \n",
    "            def image_to_geodataframe(image_array, transform, crs):\n",
    "            # Generate shapes (polygons) from the raster array\n",
    "                shapes_generator = shapes(image_array, transform=transform_)\n",
    "                \n",
    "                # Create a GeoDataFrame\n",
    "                geometries = []\n",
    "                values = []\n",
    "                for geom, value in shapes_generator:\n",
    "                    geometries.append(shape(geom))\n",
    "                    values.append(value)\n",
    "                \n",
    "                gdf = gpd.GeoDataFrame({'value': values, 'geometry': geometries}, crs=crs_)\n",
    "                \n",
    "                return gdf \n",
    "                \n",
    "            class_gdf = image_to_geodataframe(class_image, transform=transform_, crs=crs_)\n",
    "            \n",
    "            class_gdf['area'] = class_gdf.geometry.area\n",
    "            class_gdf['perimeter'] = class_gdf.geometry.length\n",
    "            class_gdf['centroid'] = class_gdf.geometry.centroid\n",
    "            # Extract latitude and longitude from centroids\n",
    "            class_gdf['longitude'] = class_gdf.centroid.x\n",
    "            class_gdf['latitude'] = class_gdf.centroid.y\n",
    "            class_gdf.set_crs(epsg=32633, inplace=True)\n",
    "            # print(class_gdf)\n",
    "            \n",
    "            # Function to calculate orientation\n",
    "            def calculate_orientation(geometry):\n",
    "                coords = np.array(geometry.exterior.coords)\n",
    "                x = coords[:, 0]\n",
    "                y = coords[:, 1]\n",
    "                \n",
    "                # Covariance matrix\n",
    "                cov_matrix = np.cov(x, y)\n",
    "                \n",
    "                # Eigenvalues and eigenvectors\n",
    "                eigvals, eigvecs = np.linalg.eig(cov_matrix)\n",
    "                \n",
    "                # Orientation angle (radians to degrees)\n",
    "                largest_eigenvec = eigvecs[:, np.argmax(eigvals)]\n",
    "                angle = np.arctan2(largest_eigenvec[1], largest_eigenvec[0])\n",
    "                return np.degrees(angle)\n",
    "            \n",
    "            # Function to calculate roundness\n",
    "            roundness = (4 * np.pi * class_gdf['area']) / (class_gdf['perimeter'] ** 2)\n",
    "            \n",
    "            # Apply orientation calculation\n",
    "            class_gdf[\"orientation\"] = class_gdf.geometry.apply(calculate_orientation)\n",
    "            class_gdf[\"roundness\"] = roundness\n",
    "            class_gdf.set_crs(epsg=32633, inplace=True)\n",
    "            gdf = class_gdf\n",
    "            # print(gdf)\n",
    "            \n",
    "            # gdf.to_parquet(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c58632-f280-44cf-acd0-cbe436395cf4",
   "metadata": {},
   "source": [
    "## Convert to dataframe for quadrants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0c98254-74f5-4c8c-87de-4a4f8b86fd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chardet\n",
    "from io import BytesIO\n",
    "from io import StringIO\n",
    "from shapely.wkb import loads as wkb_loads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9982bd3a-fd8c-4bfe-9444-970005ea28f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   value                                           geometry     area  \\\n",
      "0    3.0  POLYGON ((500375 8558925, 500375 8558875, 5004...   2500.0   \n",
      "1    2.0  POLYGON ((500275 8558875, 500275 8558825, 5003...   2500.0   \n",
      "2    3.0  POLYGON ((500425 8558825, 500425 8558775, 5004...   2500.0   \n",
      "3    4.0  POLYGON ((500825 8558275, 500825 8558225, 5007...  17500.0   \n",
      "4    4.0  POLYGON ((501175 8558075, 501175 8558025, 5010...  70000.0   \n",
      "\n",
      "   perimeter                        centroid      longitude      latitude  \\\n",
      "0      200.0          POINT (500400 8558900)  500400.000000  8.558900e+06   \n",
      "1      200.0          POINT (500300 8558850)  500300.000000  8.558850e+06   \n",
      "2      200.0          POINT (500450 8558800)  500450.000000  8.558800e+06   \n",
      "3      700.0  POINT (500871.429 8558207.143)  500871.428571  8.558207e+06   \n",
      "4     1700.0  POINT (501280.357 8557910.714)  501280.357143  8.557911e+06   \n",
      "\n",
      "   orientation  roundness  \n",
      "0   -45.000000   0.785398  \n",
      "1   -45.000000   0.785398  \n",
      "2   -45.000000   0.785398  \n",
      "3     5.194429   0.448799  \n",
      "4   -50.961191   0.304376  \n"
     ]
    }
   ],
   "source": [
    "parquet_path = \"C:/Users/jullian.williams/Desktop/HIRLOMAP/frnd/ALL_SAT/composite/RS2/Winter_reclass_test/shp/RS2_SCNA_20120203_155203_ASC_001.shp\"\n",
    "gdf = gpd.read_parquet(parquet_path)\n",
    "\n",
    "# Display the GeoDataFrame\n",
    "print(gdf.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f5a40678-8723-42bd-9529-d63ea41e1724",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"C:/Users/jullian.williams/Desktop/HIRLOMAP/frnd/ALL_SAT/composite/RS2/Winter_reclass_test/shp/\"\n",
    "output_dir = \"C:/Users/jullian.williams/Desktop/HIRLOMAP/frnd/ALL_SAT/composite/RS2/Winter_reclass_test/img2_/\"\n",
    "\n",
    "raster_path = \"C:/Users/jullian.williams/Desktop/HIRLOMAP/output_bandsSVMtrain_Pair.tif\"\n",
    "with rio.open(raster_path) as src_:\n",
    "    image_ = src_.read()  # Read all bands\n",
    "    profile_ = src_.profile  # Save metadata\n",
    "    transform_ = src_.transform\n",
    "    crs_= src_.crs\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Loop through all .tif files in the input directory\n",
    "for subdir, _, files in os.walk(input_dir):\n",
    "    for filename in files: \n",
    "        if filename.endswith(('.shp')):\n",
    "            input_path = os.path.join(subdir, filename)\n",
    "            output_path = os.path.join(output_dir, filename.replace(\".shp\", \".tif\"))\n",
    "              \n",
    "            with open(input_path) as file:\n",
    "                gdf = gpd.read_parquet(input_path)\n",
    "                # print(gdf.head())\n",
    "                \n",
    "                # Quadrants of the Fjord\n",
    "            \n",
    "                brepollen = gdf[(530000 <= gdf['longitude']) & (gdf['longitude'] <= 541628.7) & (gdf['latitude'] >= 8.542e+06) & (gdf['latitude'] <= 8.558e+06)] #533500\n",
    "                samarinvagen = gdf[(529000 <= gdf['longitude']) & (gdf['longitude'] <= 533000) & (gdf['latitude'] <= 8.545e+06) & (gdf['latitude'] <= 8.558e+06)]\n",
    "                burgerbukta = gdf[(519000 <= gdf['longitude']) & (gdf['longitude'] <= 528000) & (gdf['latitude'] >= 8.549e+06) & (gdf['latitude'] <= 8.558e+06)]\n",
    "                isbjornhamna = gdf[(513300 <= gdf['longitude']) & (gdf['longitude'] <= 517000) & (gdf['latitude'] >= 8.5470e+06) & (gdf['latitude'] <= 8.550e+06)]\n",
    "                greenlandsea = gdf[(498900 <= gdf['longitude']) & (gdf['longitude'] <= 513300)]\n",
    "                hornsund_quadrant1 = gdf[(513300 <= gdf['longitude']) & (gdf['longitude'] <= 520000) & (gdf['latitude'] >= 8.5350e+06) & (gdf['latitude'] <= 8.5470e+06)]\n",
    "                hornsund_quadrant2 = gdf[(525000 <= gdf['longitude']) & (gdf['longitude'] <= 528000) & (gdf['latitude'] >= 8.5350e+06) & (gdf['latitude'] <= 8.5490e+06)]\n",
    "                hornsund_quadrant3 = gdf[(520000 <= gdf['longitude']) & (gdf['longitude'] <= 523400) & (gdf['latitude'] >= 8.5350e+06) & (gdf['latitude'] <= 8.5440e+06)]\n",
    "                hornsund_quadrant4 = gdf[(528000 <= gdf['longitude']) & (gdf['longitude'] <= 530000) & (gdf['latitude'] >= 8.535e+06) & (gdf['latitude'] <= 8.550e+06)]\n",
    "                mainbasin = pd.concat([greenlandsea, hornsund_quadrant1, hornsund_quadrant2, hornsund_quadrant3, hornsund_quadrant4])\n",
    "                \n",
    "                \n",
    "                bpln_Nlent = brepollen[(-90 <= brepollen['orientation']) & (brepollen['orientation'] <= 0) & (brepollen['area'] >= 50000) & (brepollen['roundness'] <= 0.6)]\n",
    "                bpln_N_lent = brepollen[(0 <= brepollen['orientation']) & (brepollen['orientation'] <= 90) & (brepollen['area'] >= 50000) & (brepollen['roundness'] <= 0.6)]\n",
    "                bplnlent = pd.concat([bpln_Nlent, bpln_N_lent])\n",
    "                \n",
    "                bpln_Nrd = brepollen[(-90 <= brepollen['orientation']) & (brepollen['orientation'] <= 0) & (brepollen['area'] >= 50000) & (brepollen['roundness'] >= 0.6)]\n",
    "                bpln_N_rd = brepollen[(0 <= brepollen['orientation']) & (brepollen['orientation'] <= 90) & (brepollen['area'] >= 50000) & (brepollen['roundness'] >= 0.6)]\n",
    "                bplnrd = pd.concat([bpln_Nrd, bpln_N_rd])\n",
    "                bpln_fjord = pd.concat([bplnlent, bplnrd])\n",
    "                \n",
    "                samar_Nlent = samarinvagen[(-90 <= samarinvagen['orientation']) & (samarinvagen['orientation'] <= 0) & (samarinvagen['area'] >= 50000) & (samarinvagen['roundness'] <= 0.6)]\n",
    "                samar_N_lent = samarinvagen[(0 <= samarinvagen['orientation']) & (samarinvagen['orientation'] <= 90) & (samarinvagen['area'] >= 50000) & (samarinvagen['roundness'] <= 0.6)]\n",
    "                samarlent = pd.concat([samar_Nlent, samar_N_lent])\n",
    "                \n",
    "                samar_Nrd = samarinvagen[(-90 <= samarinvagen['orientation']) & (samarinvagen['orientation'] <= 0) & (samarinvagen['area'] >= 50000) & (samarinvagen['roundness'] >= 0.4)]\n",
    "                samar_N_rd = samarinvagen[(0 <= samarinvagen['orientation']) & (samarinvagen['orientation'] <= 90) & (samarinvagen['area'] >= 50000) & (samarinvagen['roundness'] >= 0.4)]\n",
    "                samarrd = pd.concat([samar_Nrd, samar_N_rd])\n",
    "                samar_fjord = pd.concat([samarlent, samarrd])\n",
    "                \n",
    "                burg_Nlent = burgerbukta[(-90 <= burgerbukta['orientation']) & (burgerbukta['orientation'] <= 0) & (burgerbukta['area'] >= 50000) & (burgerbukta['roundness'] <= 0.6)]\n",
    "                burg_N_lent = burgerbukta[(0 <= burgerbukta['orientation']) & (burgerbukta['orientation'] <= 90) & (burgerbukta['area'] >= 50000) & (burgerbukta['roundness'] <= 0.6)]\n",
    "                burglent = pd.concat([burg_Nlent, burg_N_lent])\n",
    "                \n",
    "                burg_Nrd = burgerbukta[(-90 <= burgerbukta['orientation']) & (burgerbukta['orientation'] <= 0) & (burgerbukta['area'] >= 50000) & (burgerbukta['roundness'] >= 0.2)]\n",
    "                burg_N_rd = burgerbukta[(0 <= burgerbukta['orientation']) & (burgerbukta['orientation'] <= 90) & (burgerbukta['area'] >= 50000) & (burgerbukta['roundness'] >= 0.2)]\n",
    "                burgrd = pd.concat([burg_Nrd, burg_N_rd])\n",
    "                burg_fjord = pd.concat([burglent, burgrd])\n",
    "                \n",
    "                isbj_Nlent = isbjornhamna[(-90 <= isbjornhamna['orientation']) & (isbjornhamna['orientation'] <= 0) & (isbjornhamna['area'] >= 50000) & (isbjornhamna['roundness'] <= 0.6)]\n",
    "                isbj_N_lent = isbjornhamna[(0 <= isbjornhamna['orientation']) & (isbjornhamna['orientation'] <= 90) & (isbjornhamna['area'] >= 50000) & (isbjornhamna['roundness'] <= 0.6)]\n",
    "                isbjlent = pd.concat([isbj_Nlent, isbj_N_lent])\n",
    "                \n",
    "                isbj_Nrd = isbjornhamna[(-90 <= isbjornhamna['orientation']) & (isbjornhamna['orientation'] <= 0) & (isbjornhamna['area'] >= 50000) & (isbjornhamna['roundness'] >= 0.6)]\n",
    "                isbj_N_rd = isbjornhamna[(0 <= isbjornhamna['orientation']) & (isbjornhamna['orientation'] <= 90) & (isbjornhamna['area'] >= 50000) & (isbjornhamna['roundness'] >= 0.6)]\n",
    "                isbjrd = pd.concat([isbj_Nrd, isbj_N_rd])\n",
    "                isbj_fjord = pd.concat([isbjlent,isbjrd])\n",
    "                \n",
    "                overlay = gdf[(gdf['area']<= 1000000000)]\n",
    "                \n",
    "                # Concatenate all to create a multipolygon shapefile\n",
    "                \n",
    "                clusterk = pd.concat([overlay,bplnlent,bplnrd,samarlent,samarrd,burglent,burgrd,isbjlent,isbjrd,mainbasin], \n",
    "                                     keys=['overlay','bplnlent','bplnrd','samarlent','samarrd','burglent','burgrd','isbjlent','isbjrd','mainbasin']).reset_index(level=0).rename(columns={'level_0': 'Source'})\n",
    "                \n",
    "                refined = clusterk[(clusterk['area'] >= 10000)]\n",
    "                \n",
    "                #Drawing the image to tif format\n",
    "                bounds = clusterk.total_bounds\n",
    "                resolution = 50  # Define resolution (e.g., 50x50 units per pixel)\n",
    "                width = int((bounds[2] - bounds[0]) / resolution)\n",
    "                height = int((bounds[3] - bounds[1]) / resolution)\n",
    "                transform = rio.transform.from_bounds(*bounds, width, height)\n",
    "                \n",
    "                # Define class colors (e.g., mapping vector attributes to raster values)\n",
    "                class_colors = {\n",
    "                    'overlay': 1,\n",
    "                    'bplnlent': 2,\n",
    "                    'bplnrd': 3,\n",
    "                    'samarlent': 4,\n",
    "                    'samarrd': 5,\n",
    "                    'burglent': 6,\n",
    "                    'burgrd': 7,\n",
    "                    'isbjlent': 8,\n",
    "                    'isbjrd': 9,\n",
    "                    'mainbasin': 10,\n",
    "                }\n",
    "                # Prepare shapes and values for rasterization\n",
    "                shapes = [(geom, class_colors[attr]) for geom, attr in zip(refined.geometry, refined['Source'])]\n",
    "                \n",
    "                # Rasterize vector data\n",
    "                rasterized = rasterize(\n",
    "                    shapes=shapes,\n",
    "                    out_shape=(height, width),\n",
    "                    transform=transform_,\n",
    "                    fill=0,  # Background value\n",
    "                    dtype=\"uint8\" \n",
    "                )\n",
    "                # plt.imshow(rasterized)\n",
    "                \n",
    "                # plt.savefig(output_path, dpi=1500,     #resoluton  \n",
    "                #            bbox_inches='tight',  # Tight layout\n",
    "                #            pad_inches=0.1,       # Padding\n",
    "                #            facecolor='white',    # Background color\n",
    "                #            edgecolor='none',     # Edge color\n",
    "                #            format='png')\n",
    "                with rio.open(output_path, \"w\", driver=\"GTiff\", height=height, width=width, count=1, dtype=rasterized.dtype, crs= src_.crs, transform=transform_,) as dst:\n",
    "                    dst.write(rasterized, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
